{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#Importing necessary Modules\n",
    "##########################\n",
    "import urllib.request as urllib2 # to download websites\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import pandas\n",
    "import collections\n",
    "\n",
    "from bs4 import BeautifulSoup # to parse the websites\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import TextBlob, Word\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from pylab import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get text data from the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-74a6007cc80d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#url = \"https://en.wikipedia.org/wiki/Picture_frame\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#url = \"https://en.wikipedia.org/wiki/Door\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Using BeautifulSoap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 642\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# Extracting the text from the page\n",
    "\n",
    "#url = \"http://edition.cnn.com/2018/01/21/politics/macron-trump-relationship-intl/index.html\"\n",
    "#url = \"http://edition.cnn.com/2018/01/29/asia/cambodia-arrests-intl/index.html?iid=ob_lockedrail_topeditorial\"\n",
    "#url = \"https://www.state.gov/r/pa/prs/ps/2017/12/276632.htm\"\n",
    "#url = \"http://www.bbc.com/news/uk-england-lancashire-42857155\"\n",
    "#url = \"http://www.bbc.com/sport/tennis/42852967\"\n",
    "#url = \"https://www.stopbullying.gov/what-is-bullying/index.html\"\n",
    "#url = \"http://www.bbc.com/news/world-europe-34818994\"\n",
    "#url = \"http://longnow.org/essays/technology-moving-too-fast/\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Child_abuse\"\n",
    "#url = \"https://www.gatesnotes.com/Saving-Lives\"\n",
    "#url = \"http://www.dictionary.com/browse/pencil?s=t\"\n",
    "\n",
    "#url = \"https://www.snopes.com/dog-saves-abandoned-newborn/\"\n",
    "#url = \"https://journals.lww.com/ajnonline/Abstract/2016/10000/Helping_Elders__Age_in_Place_.15.aspx\"\n",
    "#url = \"http://www.redcross.ca/how-we-help/international-programs/maternal--newborn-and-child-health/long-term-maternal--newborn-and-child-health-programs/maternal--newborn-and-child-health-in-africa\"\n",
    "#url = \"https://gentwenty.com/role-modeling-paving-the-way-for-younger-siblings/\"\n",
    "#url = \"http://www.missbizibee.com/golden-celebration-60th-birthday-party-ideas-mom/\"\n",
    "url = \"https://www.lifesitenews.com/news/canadian-bishops-call-for-law-to-offer-protection-for-the-lives-of-the-unbo\"\n",
    "#url = \"https://www.nature.com/scitable/knowledge/library/factors-affecting-global-climate-17079163\"\n",
    "#url = \"https://www.thesun.co.uk/news/3799392/grenfell-tower-fire-victims-death-toll-how-many-died-london/\"\n",
    "#url = \"http://darkoutpost.com/history/the-real-story-behind-the-texas-chainsaw-massacre-the-notorious-killer-ed-gein/\"\n",
    "#url = \"https://www.snopes.com/creepy-clown-haunted-house-massacre/\"\n",
    "#url = \"https://www.forbes.com/sites/jamesconca/2013/09/29/forget-eagle-deaths-wind-turbines-kill-humans/#2aed68c05467\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Silver_(color)\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Wood\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Glass\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Picture_frame\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Door\"\n",
    "html = urllib2.urlopen(url).read()\n",
    "\n",
    "# Using BeautifulSoap\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "for line in soup([\"script\", \"style\"]):     \n",
    "    line.extract()\n",
    "    \n",
    "    \n",
    "text = soup.get_text()\n",
    "\n",
    "text1 = str(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all the words and remove the stop words\n",
    "words_punt=  [word.lower() for word in word_tokenize(text) if word not in stopwords.words(\"english\")]\n",
    "#removes punctuations and special charechters\n",
    "words = [word for word in words_punt if word.isalpha()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-9ae2534fa4bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Words'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Frequency'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# frequency distribution of words\n",
    "\n",
    "# A dataframe with words and frequency\n",
    "\n",
    "\n",
    "counts = Counter(words)\n",
    "dic = {'Words':list(counts.keys()), 'Frequency':list(counts.values())}\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dic, columns=['Words','Frequency'] )\n",
    "\n",
    "# The table for frequency-plotting\n",
    "\n",
    "dfs= df.sort_values(['Frequency', 'Words'], ascending = False)\n",
    "df10 = df.head(10)\n",
    "print(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ccf31e7d4fe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcounts_noun\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Nouns'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Frequency'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf_noun\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Nouns'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Frequency'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "essays = pst\n",
    "nouns = [word for word,pos in pst\\\n",
    "\tif (pos == 'NN')]\n",
    "#print(nouns)\n",
    "#downcased = [x.lower() for x in nouns]\n",
    "#joined = \" \".join(downcased).encode('utf-8')\n",
    "#into_string = str(nouns)\n",
    "#print(nouns)\n",
    "#downcased = [x.lower() for x in nouns]\n",
    "#joined = \" \".join(downcased).encode('utf-8')\n",
    "#into_string = str(nouns)\n",
    "\n",
    "counts_noun= Counter(nouns)\n",
    "\n",
    "dic = {'Nouns':list(counts.keys()), 'Frequency':list(counts.values())}\n",
    "import pandas as pd\n",
    "df_noun= pd.DataFrame(dic, columns=['Nouns','Frequency'])\n",
    "dfs_noun = df_noun.sort_values(['Frequency', 'Nouns'], ascending = False)\n",
    "df5 = df.head(5)\n",
    "print(df5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-89c86969163b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmax_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_sentiment_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_sent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-89c86969163b>\u001b[0m in \u001b[0;36mprint_sentiment_scores\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0manalyser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_sentiment_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#    return snt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mvalence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0msentitext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;31m#text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    print(snt)\n",
    "#    return snt\n",
    "    \n",
    "    df= pd.DataFrame(snt,index=[0])\n",
    "    max_sent= df[['pos','neg','neu']].idxmax(axis=1)\n",
    "    print(max_sent[0])\n",
    "    return max_sent[0]\n",
    "#    print(snt)\n",
    "    \n",
    "\n",
    "max_sent = print_sentiment_scores(text)\n",
    "\n",
    "print(max_sent)\n",
    "\n",
    "if max_sent == 'pos':\n",
    "        print('+'*100)\n",
    "        print('The text has a positive statement.', '\\nThe most frequently 5 used nouns are: \\n\\n', Counter(nouns).most_common(5), '\\n\\nThe most frequently used positive adjectives are:\\n\\n', Counter(pos_adj_list).most_common(10), '\\nThe most frequently used positive verbs are:\\n\\n', Counter(pos_adj_list).most_common(10))\n",
    "        print('+'*100)       \n",
    "        \n",
    "elif max_sent == 'neg':\n",
    "        print('-'*100)\n",
    "        print('The text has a negative statement.\\n', '\\nThe most frequently 5 used nouns are:\\n\\n', Counter(nouns).most_common(5), '\\n\\nThe most frequently used negative adjectives are:\\n\\n', Counter(neg_adj_list).most_common(10), '\\nThe most frequently used negative verbs are:\\n\\n', Counter(neg_adj_list).most_common(10))\n",
    "        print('-'*100)\n",
    "            \n",
    "elif max_sent == 'neu':\n",
    "        print('.'*100)\n",
    "        print('The text has a neutral statement.\\n', '\\nThe most frequently 5 used nouns are:\\n\\n', Counter(nouns).most_common(5), '\\n\\nThe most frequently used neutral adjectives are:\\n\\n', Counter(neu_adj_list).most_common(10), '\\n\\nThe most frequently used neutral verbs are:\\n\\n', Counter(neu_adj_list).most_common(10))\n",
    "        print('.'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = df10['Words']\n",
    "y_pos = np.arange(len(items))\n",
    "scores = df10['Frequency']\n",
    "plt.barh(y_pos, scores)\n",
    "plt.title('Frequencies of words presented by a diagram')\n",
    "plt.ylabel('Words')\n",
    "plt.yticks(y_pos, items)\n",
    "plt.xlabel('Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and Negative sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: ['rescue', 'justice', 'hopes', 'security', 'kind', 'safety', 'safety', 'amazing', 'kind', 'love', 'please', 'pray', 'shared', 'wonderful', 'kind', 'generous', 'help', 'great', 'humour', 'play', 'loving', 'fan', 'devoted', 'huge', 'beloved', 'kind', 'energetic', 'generous', 'good', 'loved', 'friends', 'safe', 'heaven', 'adored', 'friends', 'love', 'inspiration', 'active', 'wealth', 'original', 'honour', 'loved', 'thank', 'courageously', 'play', 'engaged', 'lovers', 'generous', 'caring', 'loving', 'friend', 'great', 'friends', 'beautiful', 'loving', 'compassion', 'devotion', 'friends', 'kind', 'loving', 'generous', 'thankful', 'kind', 'handsome', 'sweet', 'kind', 'loving', 'generous', 'thankful', 'love', 'perfect', 'pride', 'good', 'positive', 'comfort', 'loved', 'accept', 'better', 'love', 'survived', 'loved', 'beautiful', 'joyful', 'intelligent', 'kind', 'dedicated', 'dignity', 'warmth', 'friends', 'creativity', 'joy', 'inspiration', 'friends', 'wonderful', 'precious', 'smiling', 'wonderful', 'precious', 'smiling', 'privilege', 'caring', 'hand', 'help', 'loved', 'smile', 'help', 'rescued', 'hero', 'hero', 'help', 'praise', 'compassion', 'party', 'alive', 'safety', 'safety', 'help', 'ease', 'perfect', 'pardon', 'party', 'agree', 'please', 'please']\n",
      "Negative: ['fire', 'victims', 'died', 'block', 'victims', 'fire', 'victims', 'died', 'block', 'trapped', 'block', 'devastating', 'fire', 'block', 'dead', 'broke', 'trapped', 'died', 'fire', 'fire', 'died', 'victims', 'fire', 'sick', 'broken', 'fraud', 'disaster', 'offences', 'fraud', 'offences', 'fire', 'offences', 'fire', 'victims', 'dead', 'victims', 'funeral', 'died', 'block', 'tragically', 'devastated', 'victims', 'block', 'raging', 'losing', 'death', 'suffered', 'block', 'catastrophic', 'fire', 'emergency', 'died', 'fire', 'fire', 'death', 'fire', 'died', 'block', 'victims', 'tragedy', 'died', 'choking', 'fire', 'heartbroken', 'sadly', 'lost', 'fire', 'heartbroken', 'tragedy', 'tragic', 'fears', 'desperate', 'raging', 'block', 'tragically', 'dead', 'sadness', 'died', 'tragic', 'dead', 'tragic', 'dead', 'died', 'dead', 'sorrow', 'loss', 'tragically', 'lost', 'dead', 'died', 'heartbreaking', 'tragedy', 'dead', 'block', 'died', 'fire', 'disaster', 'frantically', 'devastating', 'tragedy', 'grieving', 'sadly', 'lost', 'grieving', 'dead', 'contagious', 'fire', 'death', 'lost', 'fire', 'fire', 'broke', 'block', 'trapped', 'desperate', 'trapped', 'screaming', 'fire', 'horrified', 'block', 'fire', 'emergency', 'broke', 'grim', 'shame', 'dangerous', 'fire', 'victims', 'nastier', 'haunted', 'fire', 'suicide', 'horror', 'death', 'wrong', 'fire', 'frantic', 'desperate', 'failed', 'fire', 'faulty', 'failed', 'fire', 'block', 'fatal', 'fire', 'death', 'fire', 'crime', 'devastated', 'axed', 'cruelly', 'dead', 'worse', 'drunk', 'disgusting', 'forced', 'warning', 'cancer', 'starve', 'death', 'mistakes', 'complaints']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pwl=[]\n",
    "nwl=[]\n",
    "\n",
    "# Setting the value of the words,the less it is, the more words will show up, but it won't be so accurate that way \n",
    "\n",
    "for word in words:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.3:\n",
    "        pwl.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.3:\n",
    "        nwl.append(word)\n",
    "    \n",
    "print('Positive:', pwl)\n",
    "print('Negative:', nwl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part of speech tagging\n",
    "pst= nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbs in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['died', 'content', 'teamnews', 'leading', 'diversified', 'postthe', 'licensingadvertisingcontact', 'uscommissioning', 'hubtopic', 'footballall', 'grenfell', 'died', 'blaze', 'trapped', 'smoke', 'block', 'devastating', 'ripped', 'left', 'took', 'continues', 'following', 'broke', 'know', 'smoke', 'hindered', 'continued', 'trapped', 'building', 'died', 'revealed', 'died', 'including', 'named', 'mohammed', 'saye', 'opened', 'retired', 'appointed', 'tower', 'inquiry', 'produce', 'revealed', 'considering', 'pursuing', 'added', 'thought', 'involving', 'claimed', 'following', 'building', 'investigated', 'said', 'envisage', 'come', 'involve', 'offences', 'fire', 'manslaughter', 'added', 'fire', 'ripped', 'confirmed', 'mohammed', 'victim', 'identified', 'given', 'fled', 'called', 'adding', 'gave', 'love', 'came', 'aims', 'khadija', 'confirmed', 'died', 'tower', 'sent', 'saying', 'engulfed', 'cut', 'take', 'shared', 'khadija', 'described', 'devastated', 'said', 'soul', 'missed', 'remain', 'met', 'identified', 'tony', 'lived', 'engulfed', 'raging', 'said', 'left', 'filled', 'meet', 'help', 'play', 'loving', 'husband', 'mohamed', 'plunged', 'going', 'suffered', 'landing', 'stephen', 'said', 'opened', 'adjourned', 'took', 'visiting', 'seen', 'coming', 'landed', 'epa', 'died', 'found', 'morocco', 'lived', 'found', 'given', 'identified', 'became', 'tried', 'found', 'paying', 'said', 'died', 'identified', 'discovered', 'gutted', 'lived', 'found', 'died', 'choking', 'fire', 'named', 'lived', 'found', 'told', 'vanished', 'attempted', 'identified', 'said', 'beloved', 'taken', 'miss', 'loved', 'miss', 'know', 'looking', 'identified', 'identified', 'following', 'sheila', 'said', 'adored', 'gave', 'cycling', 'performing', 'swimming', 'amassed', 'wrote', 'created', 'took', 'gottardi', 'calls', 'revealed', 'lived', 'marco', 'spent', 'telling', 'loved', 'said', 'done', 'attempted', 'engaged', 'told', 'see', 'raging', 'lived', 'lived', 'said', 'caring', 'loving', 'missed', 'biruk', 'listed', 'missing', 'identified', 'grenfell', 'named', 'zainu', 'said', 'notified', 'died', 'tower', 'described', 'loving', 'identified', 'met', 'listed', 'missing', 'inquest', 'revealed', 'remains', 'confirmed', 'died', 'tweeted', 'whereabouts', 'vanished', 'confirmed', 'said', 'remembered', 'friends', 'left', 'jamal', 'paid', 'firdaws', 'hashim', 'lost', 'named', 'died', 'described', 'hearted', 'confirmed', 'said', 'hearted', 'hearted', 'knew', 'added', 'met', 'used', 'perfect', 'pride', 'ended', 'tormenting', 'gives', 'knowing', 'loved', 'accept', 'love', 'yahya', 'identified', 'believed', 'took', 'ali', 'separated', 'identified', 'said', 'stopped', 'told', 'breathe', 'shut', 'released', 'said', 'aged', 'died', 'following', 'survived', 'grandchildren', 'loved', 'missed', 'marjorie', 'appealed', 'went', 'find', 'following', 'confirmed', 'devastating', 'grieving', 'said', 'worked', 'beautiful', 'hearted', 'dedicated', 'carried', 'extended', 'taken', 'missed', 'deborah', 'named', 'described', 'smiling', 'helping', 'deborah', 'identified', 'lost', 'paid', 'saying', 'smiling', 'helping', 'debbie', 'missed', 'el', 'released', 'grieving', 'identified', 'said', 'caring', 'lend', 'asked', 'loved', 'etched', 'logan', 'fire', 'attributed', 'escaped', 'blaze', 'put', 'lost', 'named', 'came', 'identified', 'named', 'happened', 'broke', 'spreading', 'cover', 'thought', 'blaze', 'began', 'trapped', 'trapped', 'building', 'screaming', 'took', 'said', 'see', 'waving', 'windows', 'called', 'rescued', 'jumping', 'building', 'sent', 'west', 'battled', 'sent', 'alerted', 'seen', 'filling', 'blaze', 'broke', 'opened', 'evacuated', 'faced', 'removing', 'gutted', 'covered', 'cladding', 'replaced', 'listed', 'say', 'think', 'given', 'mps', 'risk', 'fire', 'grenfell', 'set', 'run', 'praise', 'archbishop', 'brits', 'kavanagh', 'emerged', 'grenfell', 'passing', 'passes', 'haunted', 'witnesses', 'grenfell', 'counting', 'proves', 'grenfell', 'give', 'reveal', 'christmas', 'harrowing', 'taken', 'showed', 'building', 'concrete', 'made', 'erupted', 'exit', 'living', 'jumped', 'avoid', 'chilling', 'released', 'revealed', 'painstaking', 'following', 'recover', 'said', 'cladding', 'attached', 'failed', 'cops', 'looking', 'started', 'tested', 'found', 'failed', 'steps', 'taken', 'tearing', 'gutted', 'coated', 'forensic', 'continue', 'ease', 'building', 'escaped', 'put', 'continue', 'opens', 'opens', 'whatsapp', 'opens', 'firecommentsmost', 'devastated', 'axed', 'swallow', 'laura', 'withdrawn', 'leaving', 'tried', 'wake', 'tots', 'dad', 'found', 'cuddling', 'worse', 'show', 'find', 'blowing', 'know', 'yobs', 'was', 'disgusting', 'forced', 'warning', 'starve', 'went', 'opens', 'opens', 'whatsapp', 'opens', 'continuing', 'change', 'find', 'licensingadvertisingcontact', 'uscommissioning', 'hubtopic', 'limited', 'london', 'registered', 'limited', 'provided', 'limited', 'accordance', 'inquire', 'reproduce', 'contact', 'see', 'map', 'regulated', 'strive', 'make', 'make']\n"
     ]
    }
   ],
   "source": [
    "mylist_verbs = []\n",
    "for list in pst:\n",
    "    (a,b) = list\n",
    "    if b in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "         mylist_verbs.append(str(list[0]))\n",
    "print(mylist_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjectives in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many', 'west', 'ineditionsscottish', 'sunirish', 'scottish', 'irish', 'saverssun', 'conditionseditorial', 'inuk', 'scottish', 'sunirish', 'showbiznewsfabulousmoneymotorstraveltechdear', 'newsuk', 'tower', 'many', 'west', 'unable', 'west', 'amupdated', 'west', 'dead', 'homeless', 'inquest', 'early', 'getty', 'burn', 'many', 'stillborn', 'first', 'syrian', 'al', 'ali', 'public', 'public', 'interim', 'september', 'individual', 'broader', 'corporate', 'detective', 'chief', 'sick', 'rifled', 'possible', 'lower', 'corporate', 'individual', 'west', 'london', 'dead', 'alhajali', 'syrian', 'funeral', 'uk', 'amazing', 'uk', 'local', 'please', 'tower', 'short', 'flat', 'mum', 'mary', 'mary', 'mary', 'saye', 'wonderful', 'incredible', 'behalf', 'anthony', 'tony', 'west', 'void', 'generous', 'great', 'face', 'practical', 'tony', 'real', 'boys', 'sixth', 'due', 'devoted', 'multiple', 'huge', 'inquest', 'catastrophic', 'tower', 'flat', 'flat', 'unresponsive', 'provisional', 'separated', 'smoke', 'flat', 'tower', 'paulous', 'isaac', 'youngest', 'little', 'dental', 'heartbroken', 'energetic', 'generous', 'little', 'good', 'god', 'safe', 'dental', 'lived', 'dental', 'official', 'sheila', 'many', 'much', 'active', 'local', 'yoga', 'daily', 'regularly', 'old', 'alternative', 'poetry', 'philosophical', 'political', 'original', 'lost', 'early', 'trevisan', 'trevisan', 'final', 'tragic', 'final', 'mum', 'desperate', 'blaze', 'pa', 'tower', 'generous', 'sister', 'deen', 'deen', 'tower', 'dead', 'great', 'sadness', 'tragic', 'beautiful', 'dead', 'found', 'hamid', 'dead', 'iranian', 'dead', 'wit', 'sorrow', 'pa', 'tribute', 'pa', 'mum', 'dead', 'blaze', 'teen', 'polite', 'generous', 'thankful', 'victim', 'yahya', 'handsome', 'sweet', 'polite', 'generous', 'thankful', 'islam', 'ignorant', 'good', 'positive', 'tiny', 'type', 'i', 'jamal', 'dead', 'top', 'ali', 'yawar', 'yawar', 'much', 'got', 'lift', 'stop', 'mr', 'vital', 'vital', 'hospital', 'vanished', 'old', 'textile', 'many', 'joyful', 'independent', 'sensitive', 'warmth', 'many', 'uk', 'lamprell', 'pa', 'wonderful', 'precious', 'tribute', 'wonderful', 'precious', 'mum', 'dead', 'loveable', 'young', 'many', 'contagious', 'logan', 'stillborn', 'flat', 'pregnant', 'induced', 'total', 'large', 'latimer', 'second', 'top', 'tower', 'least', 'desperate', 'large', 'horrified', 'london', 'sky', 'west', 'nearby', 'shelter', 'grim', 'tower', 'latest', 'grenfell', 'dangerous', 'national', 'whirlpool', 'tumble', 'london', 'stoic', 'new', 'nastier', 'divisive', 'suicide', 'wrong', 'young', 'charred', 'frantic', 'turned', 'black', 'north', 'many', 'desperate', 'burnt', 'alive', 'subsequent', 'possible', 'faulty', 'fitted', 'external', 'next', 'protective', 'eventual', 'fatal', 'perfect', 'forensic', 'click', 'new', 'new', 'new', 'popularthe', 'cole', 'brendan', 'latest', 'plummer', 'egyptian', 'dead', 'cambodia', 'drunk', 'european', 'trick', 'exclusiveraw', 'hyde', 'feral', 'debenhams', 'higher', 'skin', 'teen', 'mum', 'benderclick', 'new', 'new', 'new', 'agree', 'following', 'conditionseditorial', 'registered', 'standard', 'material', 'online', 'content', 'independent']\n"
     ]
    }
   ],
   "source": [
    "mylist_adj = []\n",
    "for list in pst:\n",
    "    (a,b) = list\n",
    "    if b in ['JJ', 'JJR', 'JJS']:\n",
    "         mylist_adj.append(str(list[0]))\n",
    "print(mylist_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouns in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grenfell', 'tower', 'fire', 'victims', 'people', 'london', 'tower', 'block', 'blaze', 'jump', 'sun', 'news', 'companycloseyour', 'sunsign', 'sunsun', 'betssun', 'bingodream', 'corp', 'network', 'companies', 'worlds', 'media', 'news', 'education', 'information', 'sunthe', 'sunthe', 'sunthe', 'sundream', 'teamsun', 'betssun', 'bingohols', 'giftsnews', 'york', 'sunterms', 'complaintsclarifications', 'correctionsnews', 'termshelp', 'storytopsign', 'edition', 'sunsun', 'betssun', 'bingodream', 'teamsearchhomefootballsporttv', 'deidretopics', 'newsworld', 'newspoliticsopinionhealth', 'newsfire', 'victims', 'fire', 'victims', 'people', 'london', 'tower', 'block', 'blaze', 'families', 'homes', 'escape', 'flames', 'london', 'ellie', 'cambridge', 'december', 'ama', 'fire', 'grenfell', 'tower', 'block', 'london', 'people', 'hundreds', 'investigations', 'recovery', 'operations', 'months', 'blaze', 'hours', 'june', 'images', 'rescue', 'efforts', 'fires', 'families', 'people', 'grenfell', 'tower', 'fire', 'police', 'months', 'fire', 'people', 'baby', 'victims', 'refugee', 'haj', 'artist', 'khadija', 'inquiry', 'cause', 'spread', 'fire', 'months', 'lord', 'justice', 'appeal', 'sir', 'martin', 'lead', 'grenfell', 'hopes', 'report', 'easter', 'police', 'manslaughter', 'charges', 'manslaughter', 'investigation', 'inspector', 'matt', 'bonner', 'looters', 'flats', 'cases', 'fraud', 'people', 'money', 'disaster', 'thefts', 'flats', 'levels', 'security', 'kind', 'stuff', 'i', 'across', 'offences', 'fraud', 'misconduct', 'health', 'safety', 'breaches', 'breaches', 'safety', 'regulations', 'course', 'offences', 'level', 'dci', 'bonner', 'rex', 'features', 'grenfell', 'kensington', 'june', 'victims', 'people', 'victims', 'refugee', 'mohammad', 'engineering', 'student', 'syria', 'years', 'family', 'kind', 'person', 'everyone', 'ambitions', 'life', 'khadija', 'saye', 'saye', 'people', 'blaze', 'grenfell', 'artist', 'message', 'pray', 'flames', 'block', 'career', 'floor', 'mendy', 'mendy', 'pa', 'press', 'association', 'mendy', 'daughter', 'khadija', 'mum', 'sister', 'aunt', 'family', 'members', 'family', 'sister', 'betty', 'jackson', 'heart', 'pure', 'kind', 'life', 'time', 'disson', 'police', 'disson', 'grenfell', 'tower', 'blaze', 'victims', 'disson', 'floor', 'london', 'tower', 'block', 'inferno', 'statement', 'family', 'losing', 'person', 'people', 'sense', 'humour', 'jokes', 'boys', 'family', 'man', 'life', 'fulham', 'fan', 'sons', 'grandchildren', 'september', 'abufars', 'son', 'death', 'burning', 'building', 'visit', 'mum', 'injuries', 'foot', 'block', 'coroners', 'officer', 'hamilton', 'circumstances', 'fire', 'hold', 'grenfell', 'mr', 'ibrahim', 'mother', 'time', 'emergency', 'services', 'building', 'foot', 'khadija', 'khalloufi', 'khadija', 'khalloufi', 'fire', 'fumes', 'khadija', 'khalloufi', 'tower', 'june', 'fire', 'fighters', 'home', 'address', 'cause', 'death', 'inhalation', 'fire', 'fumes', 'fingerprints', 'husband', 'sabah', 'abdullah', 'escape', 'building', 'police', 'grass', 'tribute', 'half', 'partner', 'everything', 'sebbar', 'inhalation', 'dna', 'abdeslam', 'floor', 'block', 'isaac', 'body', 'paulos', 'family', 'floor', 'floor', 'victims', 'isaac', 'fumes', 'youngster', 'isaac', 'shawo', 'family', 'floor', 'floor', 'westminster', 'coroner', 'court', 'dense', 'fumes', 'family', 'escape', 'blaze', 'records', 'family', 'isaac', 'son', 'kind', 'boy', 'boy', 'friends', 'family', 'heaven', 'sheila', 'epa', 'sheila', 'records', 'floor', 'grenfell', 'records', 'identification', 'family', 'resident', 'grenfell', 'years', 'family', 'friends', 'inspiration', 'member', 'community', 'sheila', 'london', 'kensington', 'leisure', 'centre', 'years', 'exploration', 'lifetime', 'wealth', 'knowledge', 'thoughts', 'artwork', 'fire', 'grenfell', 'tower', 'family', 'heartbroken', 'senseless', 'tragedy', 'honour', 'name', 'gloria', 'marco', 'gottardi', 'pa', 'press', 'association', 'gloria', 'fiance', 'marco', 'phone', 'parents', 'architecture', 'graduate', 'gloria', 'floor', 'grenfell', 'partner', 'couple', 'moments', 'parents', 'gloria', 'thank', 'marco', 'play', 'fears', 'lovers', 'families', 'stairs', 'berkti', 'haftom', 'press', 'association', 'berkti', 'haftom', 'block', 'son', 'mum', 'berkti', 'son', 'family', 'mother', 'partner', 'aunty', 'friend', 'zainab', 'deen', 'jeremiah', 'zainab', 'people', 'victim', 'pa', 'press', 'association', 'son', 'jeremiah', 'amongst', 'parents', 'maria', 'deen', 'confirm', 'police', 'zainab', 'deen', 'grenfell', 'friends', 'lady', 'son', 'jeremiah', 'police', 'tragic', 'details', 'toddler', 'body', 'mother', 'kani', 'twitter', 'hamid', 'kani', 'hamid', 'kani', 'floor', 'relatives', 'appeals', 'information', 'police', 'family', 'hamid', 'compassion', 'devotion', 'family', 'words', 'loss', 'way', 'hashim', 'kedir', 'yahya', 'hashim', 'firdwas', 'hashim', 'press', 'association', 'yahya', 'aunt', 'teenager', 'epa', 'life', 'blaze', 'press', 'association', 'nura', 'jemal', 'hashim', 'kedir', 'members', 'family', 'kind', 'loving', 'pure', 'boy', 'speaking', 'behalf', 'family', 'yahya', 'aunt', 'kind', 'pure', 'kind', 'loving', 'pure', 'boy', 'i', 'everyone', 'fall', 'love', 'politeness', 'pure', 'heartedness', 'example', 'muslim', 'humanity', 'heartbreaking', 'statement', 'mind', 'see', 'anything', 'tragedy', 'thing', 'comfort', 'fact', 'family', 'choice', 'fact', 'place', 'i', 'end', 'miss', 'parents', 'hashim', 'kedir', 'sister', 'firdaws', 'brother', 'yaqub', 'apartment', 'tower', 'block', 'inferno', 'hold', 'jafari', 'twitter', 'jafari', 'family', 'floor', 'mr', 'jafari', 'authorities', 'son', 'wife', 'daughter', 'lift', 'floor', 'paper', 'smoke', 'doors', 'till', 'ground', 'floor', 'statement', 'behalf', 'family', 'ali', 'jafari', 'caught', 'fire', 'grenfell', 'tower', 'mr', 'jafari', 'wife', 'children', 'family', 'wider', 'marjorie', 'pa', 'press', 'association', 'victim', 'grenfell', 'tower', 'disaster', 'family', 'information', 'hospital', 'blaze', 'victim', 'tragedy', 'family', 'years', 'industry', 'years', 'intelligent', 'kind', 'individual', 'life', 'children', 'dignity', 'family', 'friends', 'creativity', 'joy', 'life', 'inspiration', 'world', 'family', 'friends', 'press', 'association', 'deborah', 'lamprell', 'daughter', 'others', 'lamprell', 'victim', 'life', 'deborah', 'mum', 'statement', 'daughter', 'others', 'privilege', 'meet', 'yasin', 'wahabi', 'pa', 'press', 'association', 'yasin', 'el', 'wahabi', 'identity', 'family', 'yasin', 'el', 'statement', 'family', 'yasin', 'man', 'hand', 'anyone', 'help', 'smile', 'minds', 'hearts', 'gomes', 'hospital', 'hours', 'death', 'andreia', 'gomes', 'floor', 'husband', 'daughters', 'months', 'time', 'coma', 'unaware', 'son', 'logan', 'days', 'people', 'request', 'families', 'fire', 'fire', 'grenfell', 'tower', 'road', 'tube', 'station', 'morning', 'june', 'area', 'floor', 'floor', 'homes', 'block', 'people', 'hours', 'people', 'heard', 'help', 'fire', 'hold', 'witnesses', 'people', 'sheets', 'london', 'ambulance', 'service', 'crews', 'medics', 'tower', 'block', 'firefighters', 'fire', 'engines', 'scene', 'emergency', 'services', 'smoke', 'london', 'miles', 'hours', 'community', 'centres', 'churches', 'people', 'homes', 'firefighters', 'task', 'bodies', 'grenfell', 'towertower', 'shame', 'towers', 'goods', 'appliances', 'database', 'councils', 'millionaires', 'families', 'buildings', 'bill', 'homes', 'dryers', 'hero', 'run', 'hero', 'grenfell', 'tower', 'crew', 'marathon', 'help', 'victims', 'brits', 'canterbury', 'hails', 'compassion', 'year', 'message', 'trevor', 'labour', 'party', 'ashes', 'buck', 'grenfell', 'firm', 'responsibility', 'homes', 'council', 'blaze', 'grenfell', 'fire', 'survivors', 'times', 'horror', 'process', 'grenfell', 'death', 'toll', 'conspiracies', 'kids', 'tv', 'message', 'grenfell', 'fire', 'survivors', 'channel', 'christmas', 'speech', 'xmas', 'tribute', 'adele', 'number', 'grenfell', 'tribute', 'song', 'footage', 'grenfell', 'tower', 'remains', 'walls', 'stairwell', 'hundreds', 'residents', 'bid', 'escape', 'inferno', 'blaze', 'kensington', 'building', 'people', 'windows', 'video', 'cops', 'work', 'carnage', 'bodies', 'investigators', 'insulation', 'building', 'safety', 'tests', 'manslaughter', 'charges', 'fire', 'hotpoint', 'fridge', 'freezer', 'authorities', 'buildings', 'dozens', 'areas', 'britain', 'cladding', 'panels', 'fire', 'safety', 'tests', 'start', 'tower', 'block', 'year', 'tower', 'wrap', 'help', 'investigations', 'deconstruction', 'flats', 'building', 'fire', 'condition', 'police', 'death', 'toll', 'grenfell', 'tower', 'fire', 'teams', 'crime', 'scene', 'months', 'share', 'twitter', 'window', 'click', 'share', 'facebook', 'window', 'click', 'share', 'window', 'commentstopicsexplainersgrenfell', 'tower', 'shoulder', 'cole', 'years', 'updates', 'brit', 'tourist', 'jailexclusiveslap', 'fez', 'egypt', 'jail', 'brit', 'pardon', 'family', 'spent', 'hours', 'body', 'boozy', 'brits', 'pics', 'brits', 'party', 'spotswater', 'man', 'discovers', 'town', 'ransom', 'kids', 'run', 'apology', 'mum', 'baby', 'toiletvape', 'vapers', 'risk', 'cancer', 'heart', 'disease', 'bones', 'let', 'baby', 'death', 'share', 'twitter', 'window', 'click', 'share', 'facebook', 'window', 'click', 'share', 'window', 'use', 'site', 'use', 'cookies', 'sunservicessign', 'sunterms', 'complaintsclarifications', 'correctionsnews', 'termshelp', 'group', 'newspapers', 'england', 'office', 'london', 'bridge', 'street', 'sun', 'sun', 'sun', 'online', 'trademarks', 'trade', 'news', 'group', 'newspapers', 'service', 'news', 'group', 'newspapers', 'terms', 'conditions', 'privacy', 'cookie', 'policy', 'licence', 'visit', 'syndication', 'site', 'view', 'press', 'pack', 'inquiries', 'sun', 'please', 'use', 'site', 'sun', 'website', 'press', 'standards', 'organisation', 'journalists', 'accuracy', 'occasion', 'mistakes', 'details', 'complaints', 'policy', 'complaint', 'please', 'click']\n"
     ]
    }
   ],
   "source": [
    "mylist_nouns = []\n",
    "for list in pst:\n",
    "    (a,b) = list\n",
    "    if b in ['NN', 'NNP', 'NNS', 'NNPS']:\n",
    "         mylist_nouns.append(str(list[0]))\n",
    "print(mylist_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and Negative verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : ['love', 'shared', 'help', 'play', 'loving', 'beloved', 'loved', 'adored', 'loved', 'engaged', 'caring', 'loving', 'loving', 'friends', 'perfect', 'pride', 'loved', 'accept', 'love', 'survived', 'loved', 'beautiful', 'dedicated', 'smiling', 'smiling', 'caring', 'loved', 'rescued', 'praise', 'ease']\n",
      "Negative : ['died', 'died', 'trapped', 'block', 'devastating', 'broke', 'trapped', 'died', 'died', 'offences', 'fire', 'fire', 'died', 'devastated', 'raging', 'suffered', 'died', 'died', 'died', 'choking', 'fire', 'raging', 'died', 'died', 'lost', 'died', 'died', 'devastating', 'grieving', 'lost', 'grieving', 'fire', 'lost', 'broke', 'trapped', 'trapped', 'screaming', 'broke', 'fire', 'haunted', 'failed', 'failed', 'devastated', 'axed', 'worse', 'disgusting', 'forced', 'warning', 'starve']\n",
      "Neutral : ['content', 'teamnews', 'leading', 'diversified', 'postthe', 'licensingadvertisingcontact', 'uscommissioning', 'hubtopic', 'footballall', 'grenfell', 'blaze', 'smoke', 'ripped', 'left', 'took', 'continues', 'following', 'know', 'smoke', 'hindered', 'continued', 'building', 'revealed', 'including', 'named', 'mohammed', 'saye', 'opened', 'retired', 'appointed', 'tower', 'inquiry', 'produce', 'revealed', 'considering', 'pursuing', 'added', 'thought', 'involving', 'claimed', 'following', 'building', 'investigated', 'said', 'envisage', 'come', 'involve', 'manslaughter', 'added', 'ripped', 'confirmed', 'mohammed', 'victim', 'identified', 'given', 'fled', 'called', 'adding', 'gave', 'came', 'aims', 'khadija', 'confirmed', 'tower', 'sent', 'saying', 'engulfed', 'cut', 'take', 'khadija', 'described', 'said', 'soul', 'missed', 'remain', 'met', 'identified', 'tony', 'lived', 'engulfed', 'said', 'left', 'filled', 'meet', 'husband', 'mohamed', 'plunged', 'going', 'landing', 'stephen', 'said', 'opened', 'adjourned', 'took', 'visiting', 'seen', 'coming', 'landed', 'epa', 'found', 'morocco', 'lived', 'found', 'given', 'identified', 'became', 'tried', 'found', 'paying', 'said', 'identified', 'discovered', 'gutted', 'lived', 'found', 'named', 'lived', 'found', 'told', 'vanished', 'attempted', 'identified', 'said', 'taken', 'miss', 'miss', 'know', 'looking', 'identified', 'identified', 'following', 'sheila', 'said', 'gave', 'cycling', 'performing', 'swimming', 'amassed', 'wrote', 'created', 'took', 'gottardi', 'calls', 'revealed', 'lived', 'marco', 'spent', 'telling', 'said', 'done', 'attempted', 'told', 'see', 'lived', 'lived', 'said', 'missed', 'biruk', 'listed', 'missing', 'identified', 'grenfell', 'named', 'zainu', 'said', 'notified', 'tower', 'described', 'identified', 'met', 'listed', 'missing', 'inquest', 'revealed', 'remains', 'confirmed', 'tweeted', 'whereabouts', 'vanished', 'confirmed', 'said', 'remembered', 'left', 'jamal', 'paid', 'firdaws', 'hashim', 'named', 'described', 'hearted', 'confirmed', 'said', 'hearted', 'hearted', 'knew', 'added', 'met', 'used', 'ended', 'tormenting', 'gives', 'knowing', 'yahya', 'identified', 'believed', 'took', 'ali', 'separated', 'identified', 'said', 'stopped', 'told', 'breathe', 'shut', 'released', 'said', 'aged', 'following', 'grandchildren', 'missed', 'marjorie', 'appealed', 'went', 'find', 'following', 'confirmed', 'said', 'worked', 'hearted', 'carried', 'extended', 'taken', 'missed', 'deborah', 'named', 'described', 'helping', 'deborah', 'identified', 'paid', 'saying', 'helping', 'debbie', 'missed', 'el', 'released', 'identified', 'said', 'lend', 'asked', 'etched', 'logan', 'attributed', 'escaped', 'blaze', 'put', 'named', 'came', 'identified', 'named', 'happened', 'spreading', 'cover', 'thought', 'blaze', 'began', 'building', 'took', 'said', 'see', 'waving', 'windows', 'called', 'jumping', 'building', 'sent', 'west', 'battled', 'sent', 'alerted', 'seen', 'filling', 'blaze', 'opened', 'evacuated', 'faced', 'removing', 'gutted', 'covered', 'cladding', 'replaced', 'listed', 'say', 'think', 'given', 'mps', 'risk', 'grenfell', 'set', 'run', 'archbishop', 'brits', 'kavanagh', 'emerged', 'grenfell', 'passing', 'passes', 'witnesses', 'grenfell', 'counting', 'proves', 'grenfell', 'give', 'reveal', 'christmas', 'harrowing', 'taken', 'showed', 'building', 'concrete', 'made', 'erupted', 'exit', 'living', 'jumped', 'avoid', 'chilling', 'released', 'revealed', 'painstaking', 'following', 'recover', 'said', 'cladding', 'attached', 'cops', 'looking', 'started', 'tested', 'found', 'steps', 'taken', 'tearing', 'gutted', 'coated', 'forensic', 'continue', 'building', 'escaped', 'put', 'continue', 'opens', 'opens', 'whatsapp', 'opens', 'firecommentsmost', 'swallow', 'laura', 'withdrawn', 'leaving', 'tried', 'wake', 'tots', 'dad', 'found', 'cuddling', 'show', 'find', 'blowing', 'know', 'yobs', 'was', 'went', 'opens', 'opens', 'whatsapp', 'opens', 'continuing', 'change', 'find', 'licensingadvertisingcontact', 'uscommissioning', 'hubtopic', 'limited', 'london', 'registered', 'limited', 'provided', 'limited', 'accordance', 'inquire', 'reproduce', 'contact', 'see', 'map', 'regulated', 'strive', 'make', 'make']\n",
      "[('loved', 5), ('loving', 3), ('love', 2), ('caring', 2), ('smiling', 2), ('shared', 1), ('help', 1), ('play', 1), ('beloved', 1), ('adored', 1), ('engaged', 1), ('friends', 1), ('perfect', 1), ('pride', 1), ('accept', 1), ('survived', 1), ('beautiful', 1), ('dedicated', 1), ('rescued', 1), ('praise', 1), ('ease', 1)]\n"
     ]
    }
   ],
   "source": [
    "mylist_verbs\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_verbs_list=[]\n",
    "neg_verbs_list=[]\n",
    "neu_verbs_list=[]\n",
    "\n",
    "# Setting the value of the words,the less it is, the more words will show up, but it won't be so accurate that way \n",
    "\n",
    "for word in mylist_verbs:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.3:\n",
    "        pos_verbs_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.3:\n",
    "        neg_verbs_list.append(word)\n",
    "    else:\n",
    "        neu_verbs_list.append(word)\n",
    "\n",
    "# Printing\n",
    "\n",
    "print('Positive :',pos_verbs_list)          \n",
    "print('Negative :',neg_verbs_list)\n",
    "print('Neutral :',neu_verbs_list)\n",
    "\n",
    "ca = Counter(pos_verbs_list)\n",
    "print(ca.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and Negative adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : ['amazing', 'please', 'wonderful', 'generous', 'great', 'devoted', 'huge', 'energetic', 'generous', 'good', 'safe', 'active', 'original', 'generous', 'great', 'beautiful', 'generous', 'thankful', 'handsome', 'sweet', 'generous', 'thankful', 'good', 'positive', 'joyful', 'warmth', 'wonderful', 'precious', 'wonderful', 'precious', 'alive', 'perfect', 'agree']\n",
      "Negative : ['dead', 'sick', 'dead', 'funeral', 'catastrophic', 'heartbroken', 'lost', 'tragic', 'desperate', 'dead', 'sadness', 'tragic', 'dead', 'dead', 'dead', 'sorrow', 'dead', 'dead', 'dead', 'contagious', 'desperate', 'horrified', 'grim', 'dangerous', 'nastier', 'suicide', 'wrong', 'frantic', 'desperate', 'faulty', 'fatal', 'dead', 'drunk']\n",
      "Neutral : ['many', 'west', 'ineditionsscottish', 'sunirish', 'scottish', 'irish', 'saverssun', 'conditionseditorial', 'inuk', 'scottish', 'sunirish', 'showbiznewsfabulousmoneymotorstraveltechdear', 'newsuk', 'tower', 'many', 'west', 'unable', 'west', 'amupdated', 'west', 'homeless', 'inquest', 'early', 'getty', 'burn', 'many', 'stillborn', 'first', 'syrian', 'al', 'ali', 'public', 'public', 'interim', 'september', 'individual', 'broader', 'corporate', 'detective', 'chief', 'rifled', 'possible', 'lower', 'corporate', 'individual', 'west', 'london', 'alhajali', 'syrian', 'uk', 'uk', 'local', 'tower', 'short', 'flat', 'mum', 'mary', 'mary', 'mary', 'saye', 'incredible', 'behalf', 'anthony', 'tony', 'west', 'void', 'face', 'practical', 'tony', 'real', 'boys', 'sixth', 'due', 'multiple', 'inquest', 'tower', 'flat', 'flat', 'unresponsive', 'provisional', 'separated', 'smoke', 'flat', 'tower', 'paulous', 'isaac', 'youngest', 'little', 'dental', 'little', 'god', 'dental', 'lived', 'dental', 'official', 'sheila', 'many', 'much', 'local', 'yoga', 'daily', 'regularly', 'old', 'alternative', 'poetry', 'philosophical', 'political', 'early', 'trevisan', 'trevisan', 'final', 'final', 'mum', 'blaze', 'pa', 'tower', 'sister', 'deen', 'deen', 'tower', 'found', 'hamid', 'iranian', 'wit', 'pa', 'tribute', 'pa', 'mum', 'blaze', 'teen', 'polite', 'victim', 'yahya', 'polite', 'islam', 'ignorant', 'tiny', 'type', 'i', 'jamal', 'top', 'ali', 'yawar', 'yawar', 'much', 'got', 'lift', 'stop', 'mr', 'vital', 'vital', 'hospital', 'vanished', 'old', 'textile', 'many', 'independent', 'sensitive', 'many', 'uk', 'lamprell', 'pa', 'tribute', 'mum', 'loveable', 'young', 'many', 'logan', 'stillborn', 'flat', 'pregnant', 'induced', 'total', 'large', 'latimer', 'second', 'top', 'tower', 'least', 'large', 'london', 'sky', 'west', 'nearby', 'shelter', 'tower', 'latest', 'grenfell', 'national', 'whirlpool', 'tumble', 'london', 'stoic', 'new', 'divisive', 'young', 'charred', 'turned', 'black', 'north', 'many', 'burnt', 'subsequent', 'possible', 'fitted', 'external', 'next', 'protective', 'eventual', 'forensic', 'click', 'new', 'new', 'new', 'popularthe', 'cole', 'brendan', 'latest', 'plummer', 'egyptian', 'cambodia', 'european', 'trick', 'exclusiveraw', 'hyde', 'feral', 'debenhams', 'higher', 'skin', 'teen', 'mum', 'benderclick', 'new', 'new', 'new', 'following', 'conditionseditorial', 'registered', 'standard', 'material', 'online', 'content', 'independent']\n",
      "[('generous', 5), ('wonderful', 3), ('great', 2), ('good', 2), ('thankful', 2), ('precious', 2), ('amazing', 1), ('please', 1), ('devoted', 1), ('huge', 1), ('energetic', 1), ('safe', 1), ('active', 1), ('original', 1), ('beautiful', 1), ('handsome', 1), ('sweet', 1), ('positive', 1), ('joyful', 1), ('warmth', 1), ('alive', 1), ('perfect', 1), ('agree', 1)]\n"
     ]
    }
   ],
   "source": [
    "mylist_adj\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_adj_list=[]\n",
    "neg_adj_list=[]\n",
    "neu_adj_list=[]\n",
    "\n",
    "# Setting the value of the words,the less it is, the more words will show up, but it won't be so accurate that way \n",
    "\n",
    "for word in mylist_adj:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.3:\n",
    "        pos_adj_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.3:\n",
    "        neg_adj_list.append(word)\n",
    "    else:\n",
    "        neu_adj_list.append(word)\n",
    "        \n",
    "# Printing\n",
    "\n",
    "print('Positive :',pos_adj_list)          \n",
    "print('Negative :',neg_adj_list)\n",
    "print('Neutral :',neu_adj_list)\n",
    "\n",
    "ca = Counter(pos_adj_list)\n",
    "print(ca.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and Negative nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : ['rescue', 'justice', 'hopes', 'security', 'kind', 'safety', 'safety', 'kind', 'pray', 'kind', 'humour', 'fan', 'kind', 'friends', 'heaven', 'friends', 'inspiration', 'wealth', 'honour', 'thank', 'play', 'lovers', 'friend', 'friends', 'compassion', 'devotion', 'kind', 'loving', 'kind', 'kind', 'loving', 'love', 'comfort', 'intelligent', 'kind', 'dignity', 'friends', 'creativity', 'joy', 'inspiration', 'friends', 'privilege', 'hand', 'help', 'smile', 'help', 'hero', 'hero', 'help', 'compassion', 'party', 'safety', 'safety', 'help', 'pardon', 'party', 'please', 'please']\n",
      "Negative : ['fire', 'victims', 'block', 'victims', 'fire', 'victims', 'block', 'fire', 'block', 'fire', 'fire', 'victims', 'fire', 'fraud', 'disaster', 'offences', 'fraud', 'offences', 'victims', 'victims', 'block', 'victims', 'block', 'losing', 'death', 'block', 'fire', 'emergency', 'fire', 'fire', 'death', 'fire', 'block', 'victims', 'fire', 'heartbroken', 'tragedy', 'fears', 'block', 'tragic', 'loss', 'heartbreaking', 'tragedy', 'block', 'fire', 'disaster', 'tragedy', 'death', 'fire', 'fire', 'block', 'fire', 'block', 'fire', 'emergency', 'shame', 'victims', 'fire', 'horror', 'death', 'fire', 'fire', 'fire', 'block', 'fire', 'death', 'fire', 'crime', 'cancer', 'death', 'mistakes', 'complaints']\n",
      "Neutral : ['grenfell', 'tower', 'people', 'london', 'tower', 'blaze', 'jump', 'sun', 'news', 'companycloseyour', 'sunsign', 'sunsun', 'betssun', 'bingodream', 'corp', 'network', 'companies', 'worlds', 'media', 'news', 'education', 'information', 'sunthe', 'sunthe', 'sunthe', 'sundream', 'teamsun', 'betssun', 'bingohols', 'giftsnews', 'york', 'sunterms', 'complaintsclarifications', 'correctionsnews', 'termshelp', 'storytopsign', 'edition', 'sunsun', 'betssun', 'bingodream', 'teamsearchhomefootballsporttv', 'deidretopics', 'newsworld', 'newspoliticsopinionhealth', 'newsfire', 'people', 'london', 'tower', 'blaze', 'families', 'homes', 'escape', 'flames', 'london', 'ellie', 'cambridge', 'december', 'ama', 'grenfell', 'tower', 'london', 'people', 'hundreds', 'investigations', 'recovery', 'operations', 'months', 'blaze', 'hours', 'june', 'images', 'efforts', 'fires', 'families', 'people', 'grenfell', 'tower', 'police', 'months', 'people', 'baby', 'refugee', 'haj', 'artist', 'khadija', 'inquiry', 'cause', 'spread', 'months', 'lord', 'appeal', 'sir', 'martin', 'lead', 'grenfell', 'report', 'easter', 'police', 'manslaughter', 'charges', 'manslaughter', 'investigation', 'inspector', 'matt', 'bonner', 'looters', 'flats', 'cases', 'people', 'money', 'thefts', 'flats', 'levels', 'stuff', 'i', 'across', 'misconduct', 'health', 'breaches', 'breaches', 'regulations', 'course', 'level', 'dci', 'bonner', 'rex', 'features', 'grenfell', 'kensington', 'june', 'people', 'refugee', 'mohammad', 'engineering', 'student', 'syria', 'years', 'family', 'person', 'everyone', 'ambitions', 'life', 'khadija', 'saye', 'saye', 'people', 'blaze', 'grenfell', 'artist', 'message', 'flames', 'career', 'floor', 'mendy', 'mendy', 'pa', 'press', 'association', 'mendy', 'daughter', 'khadija', 'mum', 'sister', 'aunt', 'family', 'members', 'family', 'sister', 'betty', 'jackson', 'heart', 'pure', 'life', 'time', 'disson', 'police', 'disson', 'grenfell', 'tower', 'blaze', 'disson', 'floor', 'london', 'tower', 'inferno', 'statement', 'family', 'person', 'people', 'sense', 'jokes', 'boys', 'family', 'man', 'life', 'fulham', 'sons', 'grandchildren', 'september', 'abufars', 'son', 'burning', 'building', 'visit', 'mum', 'injuries', 'foot', 'coroners', 'officer', 'hamilton', 'circumstances', 'hold', 'grenfell', 'mr', 'ibrahim', 'mother', 'time', 'services', 'building', 'foot', 'khadija', 'khalloufi', 'khadija', 'khalloufi', 'fumes', 'khadija', 'khalloufi', 'tower', 'june', 'fighters', 'home', 'address', 'cause', 'inhalation', 'fumes', 'fingerprints', 'husband', 'sabah', 'abdullah', 'escape', 'building', 'police', 'grass', 'tribute', 'half', 'partner', 'everything', 'sebbar', 'inhalation', 'dna', 'abdeslam', 'floor', 'isaac', 'body', 'paulos', 'family', 'floor', 'floor', 'isaac', 'fumes', 'youngster', 'isaac', 'shawo', 'family', 'floor', 'floor', 'westminster', 'coroner', 'court', 'dense', 'fumes', 'family', 'escape', 'blaze', 'records', 'family', 'isaac', 'son', 'boy', 'boy', 'family', 'sheila', 'epa', 'sheila', 'records', 'floor', 'grenfell', 'records', 'identification', 'family', 'resident', 'grenfell', 'years', 'family', 'member', 'community', 'sheila', 'london', 'kensington', 'leisure', 'centre', 'years', 'exploration', 'lifetime', 'knowledge', 'thoughts', 'artwork', 'grenfell', 'tower', 'family', 'senseless', 'name', 'gloria', 'marco', 'gottardi', 'pa', 'press', 'association', 'gloria', 'fiance', 'marco', 'phone', 'parents', 'architecture', 'graduate', 'gloria', 'floor', 'grenfell', 'partner', 'couple', 'moments', 'parents', 'gloria', 'marco', 'families', 'stairs', 'berkti', 'haftom', 'press', 'association', 'berkti', 'haftom', 'son', 'mum', 'berkti', 'son', 'family', 'mother', 'partner', 'aunty', 'zainab', 'deen', 'jeremiah', 'zainab', 'people', 'victim', 'pa', 'press', 'association', 'son', 'jeremiah', 'amongst', 'parents', 'maria', 'deen', 'confirm', 'police', 'zainab', 'deen', 'grenfell', 'lady', 'son', 'jeremiah', 'police', 'details', 'toddler', 'body', 'mother', 'kani', 'twitter', 'hamid', 'kani', 'hamid', 'kani', 'floor', 'relatives', 'appeals', 'information', 'police', 'family', 'hamid', 'family', 'words', 'way', 'hashim', 'kedir', 'yahya', 'hashim', 'firdwas', 'hashim', 'press', 'association', 'yahya', 'aunt', 'teenager', 'epa', 'life', 'blaze', 'press', 'association', 'nura', 'jemal', 'hashim', 'kedir', 'members', 'family', 'pure', 'boy', 'speaking', 'behalf', 'family', 'yahya', 'aunt', 'pure', 'pure', 'boy', 'i', 'everyone', 'fall', 'politeness', 'pure', 'heartedness', 'example', 'muslim', 'humanity', 'statement', 'mind', 'see', 'anything', 'thing', 'fact', 'family', 'choice', 'fact', 'place', 'i', 'end', 'miss', 'parents', 'hashim', 'kedir', 'sister', 'firdaws', 'brother', 'yaqub', 'apartment', 'tower', 'inferno', 'hold', 'jafari', 'twitter', 'jafari', 'family', 'floor', 'mr', 'jafari', 'authorities', 'son', 'wife', 'daughter', 'lift', 'floor', 'paper', 'smoke', 'doors', 'till', 'ground', 'floor', 'statement', 'behalf', 'family', 'ali', 'jafari', 'caught', 'grenfell', 'tower', 'mr', 'jafari', 'wife', 'children', 'family', 'wider', 'marjorie', 'pa', 'press', 'association', 'victim', 'grenfell', 'tower', 'family', 'information', 'hospital', 'blaze', 'victim', 'family', 'years', 'industry', 'years', 'individual', 'life', 'children', 'family', 'life', 'world', 'family', 'press', 'association', 'deborah', 'lamprell', 'daughter', 'others', 'lamprell', 'victim', 'life', 'deborah', 'mum', 'statement', 'daughter', 'others', 'meet', 'yasin', 'wahabi', 'pa', 'press', 'association', 'yasin', 'el', 'wahabi', 'identity', 'family', 'yasin', 'el', 'statement', 'family', 'yasin', 'man', 'anyone', 'minds', 'hearts', 'gomes', 'hospital', 'hours', 'andreia', 'gomes', 'floor', 'husband', 'daughters', 'months', 'time', 'coma', 'unaware', 'son', 'logan', 'days', 'people', 'request', 'families', 'grenfell', 'tower', 'road', 'tube', 'station', 'morning', 'june', 'area', 'floor', 'floor', 'homes', 'people', 'hours', 'people', 'heard', 'hold', 'witnesses', 'people', 'sheets', 'london', 'ambulance', 'service', 'crews', 'medics', 'tower', 'firefighters', 'engines', 'scene', 'services', 'smoke', 'london', 'miles', 'hours', 'community', 'centres', 'churches', 'people', 'homes', 'firefighters', 'task', 'bodies', 'grenfell', 'towertower', 'towers', 'goods', 'appliances', 'database', 'councils', 'millionaires', 'families', 'buildings', 'bill', 'homes', 'dryers', 'run', 'grenfell', 'tower', 'crew', 'marathon', 'brits', 'canterbury', 'hails', 'year', 'message', 'trevor', 'labour', 'ashes', 'buck', 'grenfell', 'firm', 'responsibility', 'homes', 'council', 'blaze', 'grenfell', 'survivors', 'times', 'process', 'grenfell', 'toll', 'conspiracies', 'kids', 'tv', 'message', 'grenfell', 'survivors', 'channel', 'christmas', 'speech', 'xmas', 'tribute', 'adele', 'number', 'grenfell', 'tribute', 'song', 'footage', 'grenfell', 'tower', 'remains', 'walls', 'stairwell', 'hundreds', 'residents', 'bid', 'escape', 'inferno', 'blaze', 'kensington', 'building', 'people', 'windows', 'video', 'cops', 'work', 'carnage', 'bodies', 'investigators', 'insulation', 'building', 'tests', 'manslaughter', 'charges', 'hotpoint', 'fridge', 'freezer', 'authorities', 'buildings', 'dozens', 'areas', 'britain', 'cladding', 'panels', 'tests', 'start', 'tower', 'year', 'tower', 'wrap', 'investigations', 'deconstruction', 'flats', 'building', 'condition', 'police', 'toll', 'grenfell', 'tower', 'teams', 'scene', 'months', 'share', 'twitter', 'window', 'click', 'share', 'facebook', 'window', 'click', 'share', 'window', 'commentstopicsexplainersgrenfell', 'tower', 'shoulder', 'cole', 'years', 'updates', 'brit', 'tourist', 'jailexclusiveslap', 'fez', 'egypt', 'jail', 'brit', 'family', 'spent', 'hours', 'body', 'boozy', 'brits', 'pics', 'brits', 'spotswater', 'man', 'discovers', 'town', 'ransom', 'kids', 'run', 'apology', 'mum', 'baby', 'toiletvape', 'vapers', 'risk', 'heart', 'disease', 'bones', 'let', 'baby', 'share', 'twitter', 'window', 'click', 'share', 'facebook', 'window', 'click', 'share', 'window', 'use', 'site', 'use', 'cookies', 'sunservicessign', 'sunterms', 'complaintsclarifications', 'correctionsnews', 'termshelp', 'group', 'newspapers', 'england', 'office', 'london', 'bridge', 'street', 'sun', 'sun', 'sun', 'online', 'trademarks', 'trade', 'news', 'group', 'newspapers', 'service', 'news', 'group', 'newspapers', 'terms', 'conditions', 'privacy', 'cookie', 'policy', 'licence', 'visit', 'syndication', 'site', 'view', 'press', 'pack', 'inquiries', 'sun', 'use', 'site', 'sun', 'website', 'press', 'standards', 'organisation', 'journalists', 'accuracy', 'occasion', 'details', 'policy', 'complaint', 'click']\n"
     ]
    }
   ],
   "source": [
    "mylist_nouns\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_nouns_list=[]\n",
    "neg_nouns_list=[]\n",
    "neu_nouns_list=[]\n",
    "\n",
    "\n",
    "# Setting the value of the words,the less it is, the more words will show up, but it won't be so accurate that way \n",
    "\n",
    "for word in mylist_nouns:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.3:\n",
    "        pos_nouns_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.3:\n",
    "        neg_nouns_list.append(word)\n",
    "    else:\n",
    "        neu_nouns_list.append(word)\n",
    "\n",
    "# Printing\n",
    "\n",
    "print('Positive :',pos_nouns_list)          \n",
    "print('Negative :',neg_nouns_list)\n",
    "print('Neutral :',neu_nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
